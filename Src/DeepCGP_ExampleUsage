# ==============================
# Tanzila Islam
# Email: tanzilamohita@gmail.com
# ===============================

import os
import glob
import numpy as np
import time

from sklearn.model_selection import train_test_split
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf

# for gpu use
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"GPU is available: {gpus}")
else:
    print("GPU is NOT available. Running on CPU.")


# ============ Configuration ============
C = 3
Data = "Maize"
META_DIR = f"../ModelMetaData_{Data}"
NPY_FOLDER = f"{META_DIR}/{Data}_Compressed_Separated/C2"
COMPRESSED_OUTPUT = f"{META_DIR}/{Data}_Compressed_Separated/C{C}"
os.makedirs(COMPRESSED_OUTPUT, exist_ok=True)

# Best config (already tuned for C3)
best_config = {
    "InputDim": 112,
    "Compress": [84, 56, 14],
    "BatchSize": 64,
    "LearningRate": 0.001,
    "Epochs": 200,
}


# ============ Autoencoder Model ============
def build_autoencoder(input_dim, compress):
    h1, h2, bottleneck = compress
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(h1, activation='relu')(input_layer)
    encoded = Dense(h2, activation='relu')(encoded)
    encoded = Dense(bottleneck, activation='sigmoid')(encoded)
    decoded = Dense(h2, activation='relu')(encoded)
    decoded = Dense(h1, activation='relu')(decoded)
    decoded = Dense(input_dim, activation='sigmoid')(decoded)
    autoencoder = Model(input_layer, decoded)
    encoder = Model(input_layer, encoded)
    return autoencoder, encoder


def compress_single_chromosome(path, best_config):
    input_dim = best_config['InputDim']
    compress = best_config['Compress']
    batch_size = best_config['BatchSize']
    lr = best_config['LearningRate']
    epochs = best_config['Epochs']

    chr_name = os.path.basename(path).replace(f'_C2.npy', '')
    print(f"\nProcessing {chr_name} ...")
    chr_data = np.load(path, allow_pickle=True).astype(np.float32)

    n_chunks = chr_data.shape[1] // input_dim
    usable = n_chunks * input_dim
    split_chunks = np.hsplit(chr_data[:, :usable], n_chunks)

    encoded_chunks = []

    for i, chunk in enumerate(split_chunks):
        print(f"â†’ Chunk {i + 1}/{len(split_chunks)}")
        x_train, x_mid = train_test_split(chunk, test_size=0.4, random_state=42)
        x_test, x_valid = train_test_split(x_mid, test_size=0.5, random_state=42)

        autoencoder, encoder = build_autoencoder(input_dim, compress)
        autoencoder.compile(optimizer=Adam(learning_rate=lr), loss='mse')
        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        autoencoder.fit(
            x_train, x_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(x_valid, x_valid),
            callbacks=[early_stop],
            verbose=0
        )

        # Encode the current chunk
        encoded = encoder.predict(chunk, batch_size=batch_size, verbose=0)
        encoded_chunks.append(encoded)

    # Combine all encoded chunks and save
    compressed_array = np.hstack(encoded_chunks)
    output_path = os.path.join(COMPRESSED_OUTPUT, f"{chr_name}_C{C}.npy")
    np.save(output_path, compressed_array)

    print(f"Saved compressed data: {output_path}")

    return os.path.basename(path)


# ============ Run Script ============
if __name__ == "__main__":
    total_start_time = time.time()

    file_paths = sorted(glob.glob(os.path.join(NPY_FOLDER, f'GSTP004_chr*_C2.npy')))
    for path in file_paths:
        chr_name = os.path.basename(path).replace(f'_C2.npy', '')
        compress_single_chromosome(path, best_config)
